{"cells":[{"cell_type":"code","execution_count":null,"id":"cf2cf167-9956-4897-a292-249bd31cdc17","metadata":{"id":"cf2cf167-9956-4897-a292-249bd31cdc17","outputId":"32beb3c3-d52b-4bb1-fd4f-34eaa1c1e49b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pandas in c:\\users\\feder\\anaconda3\\lib\\site-packages (2.1.4)\n","Requirement already satisfied: networkx in c:\\users\\feder\\anaconda3\\lib\\site-packages (3.1)\n","Requirement already satisfied: nltk in c:\\users\\feder\\anaconda3\\lib\\site-packages (3.8.1)\n","Requirement already satisfied: zstandard in c:\\users\\feder\\anaconda3\\lib\\site-packages (0.19.0)\n","Collecting orjson\n","  Obtaining dependency information for orjson from https://files.pythonhosted.org/packages/5d/67/d7837cf0ac956e3c81c67dda3e8f2ffc60dd50ffc480ec7c17f2e22a36ae/orjson-3.9.10-cp311-none-win_amd64.whl.metadata\n","  Downloading orjson-3.9.10-cp311-none-win_amd64.whl.metadata (50 kB)\n","     ---------------------------------------- 0.0/50.5 kB ? eta -:--:--\n","     --------------- ---------------------- 20.5/50.5 kB 640.0 kB/s eta 0:00:01\n","     -------------------------------------- 50.5/50.5 kB 638.6 kB/s eta 0:00:00\n","Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\feder\\anaconda3\\lib\\site-packages (from pandas) (1.26.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\feder\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\feder\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.1 in c:\\users\\feder\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n","Requirement already satisfied: click in c:\\users\\feder\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in c:\\users\\feder\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in c:\\users\\feder\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n","Requirement already satisfied: tqdm in c:\\users\\feder\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n","Requirement already satisfied: six>=1.5 in c:\\users\\feder\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: colorama in c:\\users\\feder\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n","Downloading orjson-3.9.10-cp311-none-win_amd64.whl (135 kB)\n","   ---------------------------------------- 0.0/135.0 kB ? eta -:--:--\n","   --------------------------- ------------ 92.2/135.0 kB 2.6 MB/s eta 0:00:01\n","   ---------------------------------------- 135.0/135.0 kB 2.7 MB/s eta 0:00:00\n","Installing collected packages: orjson\n","Successfully installed orjson-3.9.10\n"]}],"source":["!pip install pandas networkx nltk zstandard orjson"]},{"cell_type":"markdown","id":"af5452b2-32e6-49ea-b100-b8604425b843","metadata":{"id":"af5452b2-32e6-49ea-b100-b8604425b843"},"source":["<h1>Importing necessary libraries and downloading the ‘vader_lexicon’ package</h1>"]},{"cell_type":"code","execution_count":null,"id":"21bba60d-350d-4462-b737-e5b92f51f181","metadata":{"id":"21bba60d-350d-4462-b737-e5b92f51f181","outputId":"8fe29839-0900-4d9b-a439-fffcabcc0b0a"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package vader_lexicon to\n","[nltk_data]     C:\\Users\\feder\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package vader_lexicon is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","import nltk\n","\n","# Make sure you have downloaded the 'vader_lexicon' package\n","nltk.download('vader_lexicon')"]},{"cell_type":"markdown","id":"0d7fb9f2-018c-468d-aaee-9ecd77b9daf9","metadata":{"id":"0d7fb9f2-018c-468d-aaee-9ecd77b9daf9"},"source":["<h1> Reading and cleaning the data</h1>"]},{"cell_type":"code","execution_count":null,"id":"e6254d5d-eaf1-4b13-b7ac-8cf00df89bd1","metadata":{"id":"e6254d5d-eaf1-4b13-b7ac-8cf00df89bd1"},"outputs":[],"source":["start_year = 2007\n","end_year = 2022\n","chunksize = 10**6  # Set the chunk size\n","\n","# For each year in the specified range\n","for year in range(start_year, end_year + 1):\n","    # Create the file name\n","    file_name = f\"politics_comments.zst_{year}.csv\"\n","\n","    # Initialize an empty DataFrame for the comments\n","    df_comments = pd.DataFrame()\n","\n","    # Read the file in chunks\n","    for chunk in pd.read_csv(file_name, names=[\"author\",\"body\",\"date\",\"parent_id\",\"id\",\"score\",\"author_fullname\",\"ups\"], chunksize=chunksize):\n","        # Filter rows where 'author' or 'body' is '[deleted]' or '[removed]'\n","        chunk = chunk[~chunk['author'].isin(['[deleted]', '[removed]'])]\n","        chunk = chunk[~chunk['body'].isin(['[deleted]', '[removed]'])]\n","\n","        # Convert the 'date' column to datetime format\n","        chunk['date'] = pd.to_datetime(chunk['date'], unit='s')\n","\n","        # Add the chunk to the DataFrame\n","        df_comments = pd.concat([df_comments, chunk])\n","\n","    # Create a global variable for the DataFrame\n","    globals()[f\"politics_comments_{year}\"] = df_comments\n","\n","    # Create the file name for the submissions\n","    file_name_submissions = f\"politics_submissions.zst_{year}.csv\"\n","\n","    # Initialize an empty DataFrame for the submissions\n","    df_submissions = pd.DataFrame()\n","\n","    # Read the file in chunks\n","    for chunk in pd.read_csv(file_name_submissions, names=[\"author\",\"created_utc\",\"id\",\"num_comments\",\"title\",\"score\",\"author_fullname\",\"ups\"], chunksize=chunksize):\n","        # Filter rows where 'author' is '[deleted]' or '[removed]'\n","        chunk = chunk[~chunk['author'].isin(['[deleted]', '[removed]'])]\n","\n","        # Convert the 'created_utc' column to datetime format\n","        chunk['created_utc'] = pd.to_datetime(chunk['created_utc'], unit='s')\n","\n","        # Add the chunk to the DataFrame\n","        df_submissions = pd.concat([df_submissions, chunk])\n","\n","    # Create a global variable for the DataFrame\n","    globals()[f\"politics_submissions_{year}\"] = df_submissions\n"]},{"cell_type":"markdown","id":"8d8c723a-d5a0-478b-866b-14e2e2cac7fd","metadata":{"id":"8d8c723a-d5a0-478b-866b-14e2e2cac7fd"},"source":["<h1> Sentiment analysis and splitting the ID</h1>"]},{"cell_type":"code","execution_count":null,"id":"844f848b-13cc-4044-bdc9-a587e80336f1","metadata":{"id":"844f848b-13cc-4044-bdc9-a587e80336f1"},"outputs":[],"source":["# Create a SentimentIntensityAnalyzer object\n","sia = SentimentIntensityAnalyzer()\n","\n","def get_sentiment_scores(comment):\n","    scores = sia.polarity_scores(comment)\n","    return pd.Series(scores)\n","\n","def split_id(fullname):\n","    # Extract the type and ID from the fullname\n","    type_prefix, id_ = fullname.split('_')\n","    # Map the type prefix to a more descriptive string\n","    type_ = 'comment' if type_prefix == 't1' else 'post' if type_prefix == 't3' else 'unknown'\n","    return pd.Series([type_, id_])\n","\n","start_year = 2007\n","end_year = 2022\n","\n","# For each dataframe in that range (inclusive of start and end), the code does a series of operations\n","for year in range(start_year, end_year + 1):\n","    # Get the dataframes for the current year\n","    politics_comments = globals()[f'politics_comments_{year}']\n","    politics_submissions = globals()[f'politics_submissions_{year}']\n","\n","    # Sentiment analysis with VADER\n","    politics_comments['body'] = politics_comments['body'].fillna('').astype(str)\n","    sentiment_scores = politics_comments['body'].apply(get_sentiment_scores)\n","    sentiment_scores.columns = ['negative', 'neutral', 'positive', 'sentiment']\n","    politics_comments = pd.concat([politics_comments, sentiment_scores], axis=1)\n","\n","    # Split fullname\n","    politics_comments[['parent_type', 'parent_id']] = politics_comments['parent_id'].apply(split_id)\n","\n","    # Creation of the corresponding year's connections dataframe\n","    id_author_map_posts = politics_submissions[['id', 'author']].set_index('id').to_dict()['author']\n","    id_author_map_comments = politics_comments[['id', 'author']].set_index('id').to_dict()['author']\n","    connections = politics_comments[['author', 'id', 'parent_id', 'sentiment']].copy()\n","    connections['parent_author'] = politics_comments.apply(lambda row: id_author_map_posts.get(row['parent_id']) if row['parent_type'] == 'post' else id_author_map_comments.get(row['parent_id']), axis=1)\n","\n","    # Save the connections dataframe as a global variable\n","    globals()[f'connections_{year}'] = connections\n"]},{"cell_type":"markdown","id":"612b142c-d41d-4474-ae43-b8cbf259bc8a","metadata":{"id":"612b142c-d41d-4474-ae43-b8cbf259bc8a"},"source":["<h1> Saving the dataframes</h1>"]},{"cell_type":"code","execution_count":null,"id":"324f2522-f960-40f0-ac04-0648c98aaf09","metadata":{"id":"324f2522-f960-40f0-ac04-0648c98aaf09"},"outputs":[],"source":["import pickle\n","\n","def save_politics_dataframes(start_year, end_year):\n","    for year in range(start_year, end_year+1):\n","        for data_type in ['submissions', 'comments']:\n","            df_name = f'politics_{data_type}_{year}'\n","            df = globals()[df_name]\n","            with open(f'{df_name}.pickle', 'wb') as f:\n","                pickle.dump(df, f)\n","\n","def save_connections_dataframes(start_year, end_year):\n","    for year in range(start_year, end_year+1):\n","        df_name = f'connections_{year}'\n","        df = globals()[df_name]\n","        with open(f'{df_name}.pickle', 'wb') as f:\n","            pickle.dump(df, f)\n","\n","# Call the functions\n","save_politics_dataframes(2007, 2022)\n","save_connections_dataframes(2007, 2022)\n"]},{"cell_type":"markdown","id":"c8495a37-41d4-42f3-b4f9-b0abf867672b","metadata":{"id":"c8495a37-41d4-42f3-b4f9-b0abf867672b"},"source":["<h1>Processing the data and creating the graph</h1>"]},{"cell_type":"code","execution_count":null,"id":"d0747021-c671-474a-ba7e-073ecab620fc","metadata":{"id":"d0747021-c671-474a-ba7e-073ecab620fc"},"outputs":[],"source":["import pickle\n","import pandas as pd\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","import gc\n","\n","def process_year(year):\n","    # Name of the pickle file\n","    filename = f'connections_{year}.pickle'\n","\n","    # Open the pickle file and turn it into a dataframe\n","    with open(filename, 'rb') as f:\n","        df = pickle.load(f)\n","\n","    # Create a boolean mask for the rows to keep\n","    mask = (df['parent_author'].notna()) & (df['author'] != df['parent_author'])\n","\n","    # Apply the mask to the DataFrame\n","    df = df.loc[mask]\n","\n","    # Save the modified dataframe with the same name as the original\n","    with open(filename, 'wb') as f:\n","        pickle.dump(df, f)\n","\n","    # Create a new DataFrame grouped by 'author' and 'parent_author'\n","    df_grouped = df.groupby(['author', 'parent_author'])\n","\n","    # Create a DataFrame with 'number_of_interactions' and 'avg_sentiment'\n","    df_grouped = df_grouped.agg(\n","        number_of_interactions=('author', 'count'),\n","        avg_sentiment=('sentiment', 'mean')\n","    ).reset_index()\n","\n","    # Create a new column 'interaction_sentiment_weighted'\n","    df_grouped['interaction_sentiment_weighted'] = df_grouped['avg_sentiment'] * df_grouped['number_of_interactions']\n","\n","    # Save the grouped dataframe as a pickle\n","    with open(f'groupby_connections_{year}.pickle', 'wb') as f:\n","        pickle.dump(df_grouped, f)\n","\n","    # Create a full directed graph\n","    G_full = nx.from_pandas_edgelist(df_grouped, 'author', 'parent_author', ['number_of_interactions', 'avg_sentiment'], create_using=nx.DiGraph())\n","\n","    # Save the graph\n","    with open(f'graph_{year}.pkl', 'wb') as f:\n","        pickle.dump(G_full, f)\n","\n","    # Delete the data to save memory\n","    del df\n","    del df_grouped\n","    del G_full\n","    gc.collect()\n","\n","def process_range(start_year, end_year):\n","    for year in range(start_year, end_year + 1):\n","        process_year(year)\n","\n","# Run the process_range function with the desired range of years\n","process_range(2007, 2022)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[{"file_id":"1wn0dzJJ7i-tKRJy0qVDDI6Xbdmdu_7tk","timestamp":1704985004562}]}},"nbformat":4,"nbformat_minor":5}