{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will find:\n",
    "- Key words extraction\n",
    "- Polarization Analysis\n",
    "- Emotion Detection\n",
    "- NER Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install seaborn\n",
    "! pip install textblob\n",
    "! pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import lzma\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nrclex import NRCLex\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import re \n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change here your directory path where the dataframes are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"C:/Users/Paola/OneDrive/Desktop/Data Science/2 ANNO/Social media analytics/Progetto/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_words(text): \n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_words = pos_tag(tokens)\n",
    "    key_words = [word.lower() for word, pos in tagged_words if pos.startswith('N') or pos.startswith('J')]\n",
    "    return key_words\n",
    "\n",
    "for year in range(2007, 2023):\n",
    "    file_path = f\"{directory_path}politics_comments_{year}.lzma\"\n",
    "    \n",
    "    try:\n",
    "        with lzma.open(file_path, 'rb') as file:\n",
    "            df = pd.read_pickle(file)\n",
    "\n",
    "        ## change number of rows\n",
    "        selected_rows = df.nlargest(10000, 'score')\n",
    "\n",
    "        # Concatenate all comments into a single string\n",
    "        comments_text = ' '.join(selected_rows['body'].dropna())\n",
    "\n",
    "        # Stop-words removal\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [word.lower() for word in comments_text.split() if word.lower() not in stop_words]\n",
    "\n",
    "        key_words = extract_key_words(comments_text)\n",
    "\n",
    "        # Generate WordCloud\n",
    "        wordcloud = WordCloud(width=800, height=400, max_words=200, background_color='white').generate(' '.join(key_words))\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.title(f\"Word Cloud for {year}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        # Top 20 key words\n",
    "        key_word_counts = Counter(key_words)\n",
    "        top_key_words = key_word_counts.most_common(20)\n",
    "        print(f\"Top 20 Key Words for {year}:\")\n",
    "        for word, count in top_key_words:\n",
    "            print(f\"{word}: {count}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_political_sentiment(comment):\n",
    "    analysis = TextBlob(comment)\n",
    "    return analysis.sentiment.polarity\n",
    "\n",
    "for year in range(2007, 2023):\n",
    "    try:\n",
    "        file_path = f\"{directory_path}politics_comments_{year}.lzma\"\n",
    "\n",
    "        with lzma.open(file_path, 'rb') as file:\n",
    "            df = pd.read_pickle(file)\n",
    "\n",
    "        ## change number of rows\n",
    "        top_comments = df.nlargest(10000, 'score')\n",
    "\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        top_comments['body'] = top_comments['body'].apply(lambda x: ' '.join([word.lower() for word in x.split() if word.lower() not in stop_words]))\n",
    "\n",
    "        top_comments['political_sentiment'] = top_comments['body'].apply(get_political_sentiment)\n",
    "\n",
    "        # Crea un barplot\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.histplot(top_comments['political_sentiment'], bins=20, kde=False, color='skyblue', edgecolor='black')\n",
    "        plt.title(f'Barplot of Political Sentiment for {year}')\n",
    "        plt.xlabel('Political Sentiment Score')\n",
    "        plt.ylabel('Count')\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante l'analisi dell'anno {year}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2007, 2023):\n",
    "    try:\n",
    "        file_path = f\"{directory_path}connections_{year}.lzma\"\n",
    "\n",
    "        with lzma.open(file_path, 'rb') as file:\n",
    "            df = pd.read_pickle(file)\n",
    "\n",
    "        # Crea un barplot\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.histplot(df['sentiment'], bins=20, kde=False, color='skyblue', edgecolor='black')\n",
    "        plt.title(f'Barplot of Sentiment for {year}')\n",
    "        plt.xlabel('Sentiment Score')\n",
    "        plt.ylabel('Count')\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante l'analisi dell'anno {year}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nrclex(df):\n",
    "  text_object = NRCLex(' '.join(df['body']))\n",
    "  return text_object\n",
    "\n",
    "def get_emofreq(text_object):\n",
    "  print(text_object.affect_frequencies)\n",
    "  print('\\nThe top emotion is:')\n",
    "  print(text_object.top_emotions)\n",
    "\n",
    "def get_sentimentscores(text_object):\n",
    "  sentiment_scores = pd.DataFrame(list(text_object.raw_emotion_scores.items()))\n",
    "  sentiment_scores = sentiment_scores.rename(columns={0: \"Sentiment\", 1: \"Count\"})\n",
    "  print(sentiment_scores)\n",
    "  print('\\n')\n",
    "\n",
    "  fig = px.pie(sentiment_scores, values='Count', names='Sentiment',\n",
    "             title='Sentiment Scores',\n",
    "             hover_data=['Sentiment'])\n",
    "  fig.update_traces(textposition='inside', textinfo='percent+label')\n",
    "  fig.show()\n",
    "\n",
    "for year in range(2007, 2023):\n",
    "    try:\n",
    "        file_path = f\"{directory_path}politics_comments_{year}.lzma\"\n",
    "\n",
    "        with lzma.open(file_path, 'rb') as file:\n",
    "            df = pd.read_pickle(file)\n",
    "\n",
    "        ## change number of rows\n",
    "        top_comments = df.nlargest(10000, 'score')\n",
    "\n",
    "        print(f\"Dataframe top_comments per l'anno {year}:\")\n",
    "        print(top_comments.head())\n",
    "\n",
    "        text_object = get_nrclex(top_comments)\n",
    "        get_emofreq(text_object)\n",
    "        get_sentimentscores(text_object)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante l'analisi dell'anno {year}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esegui (se necessario) nel prompt dei comandi --> python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def perform_ner(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents if ent.label_ in ['PERSON', 'ORG']]\n",
    "    return entities\n",
    "\n",
    "for year in range(2007, 2010):\n",
    "    try:\n",
    "        file_path = f\"{directory_path}politics_comments_{year}.lzma\"\n",
    "\n",
    "        with lzma.open(file_path, 'rb') as file:\n",
    "            df = pd.read_pickle(file)\n",
    "\n",
    "        ## change number of rows\n",
    "        top_comments = df.nlargest(10000, 'score')\n",
    "\n",
    "        top_comments['ner_results'] = top_comments['body'].apply(perform_ner)\n",
    "\n",
    "        all_entities = [entity for entities_list in top_comments['ner_results'] for entity, label in entities_list]\n",
    "        entity_counts = Counter(all_entities)\n",
    "        top_n = 10\n",
    "        top_entities = entity_counts.most_common(top_n)\n",
    "\n",
    "        print(f\"Named Entity Recognition Results for the top 1000 comments in {year}:\")\n",
    "        print(top_comments[['body', 'ner_results']].head(10))\n",
    "        print(\"\\n\")\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar([entity[0] for entity in top_entities], [entity[1] for entity in top_entities])\n",
    "        plt.xlabel('Entity')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title(f'Top {top_n} Named Entities for {year}')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.show()\n",
    "\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(all_entities))\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Word Cloud for Named Entities (PERSON and ORG) - {year}')\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante l'analisi NER dell'anno {year}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
